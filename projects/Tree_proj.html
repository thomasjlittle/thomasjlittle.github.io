<!DOCTYPE html>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html lang="en">
  <head>
    <title>Thomas Little</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="/assets/css/main.css" />
  </head>

  <body class="is-preload">
    <!-- Header -->
    <section id="header">
      <header>
        <span class="image avatar"
          ><img src="/images/avatar.jpg" alt=""
        /></span>
        <h1 id="logo"><a href="#">Thomas Little</a></h1>
        <p>
          MS Stanford University<br />
          Passionate about machine learning<br />
          egg
        </p>
      </header>
      <nav id="nav">
        <ul>
          <li><a href="/index.html#one">About</a></li>
          <li><a href="/index.html#two">Education</a></li>
          <li><a href="/index.html#three" class="active">Projects</a></li>
        </ul>
        <!-- <ul>
          <li>
            <a href="GMM_project.html">Ground Motion Modeling using DNNs</a>
          </li>
          <li>
            <a href="GNN_Project.html"
              >Graph Neural Nets for Polymer Prediction</a
            >
          </li>
          <li>
            <a href="Tree_proj.html" class="active"
              >Increasing Compositionality in Small LMs</a
            >
          </li>
          <li>
            <a href="Nonlin_project.html">Nonlinear Structural Analysis</a>
          </li>
          <li><a href="/index.html">Home</a></li>
        </ul> -->
      </nav>
      <footer>
        <ul class="icons">
          <li>
            <a
              href="https://www.linkedin.com/in/thomas-little-0/"
              class="icon brands fa-linkedin"
              ><span class="label">LinkedIn</span></a
            >
          </li>
          <li>
            <a
              href="https://github.com/thomasjlittle"
              class="icon brands fa-github"
              ><span class="label">Github</span></a
            >
          </li>
          <li>
            <a
              href="mailto:thomasjlittle01@gmail.com"
              class="icon solid fa-envelope"
              ><span class="label">Email</span></a
            >
          </li>
        </ul>
      </footer>
    </section>

    <!-- Wrapper -->
    <div id="wrapper">
      <!-- Main -->
      <div id="main">
        <!-- One -->
        <section id="one">
          <button class="button" onclick="goBack()">Back</button>
          <script>
            function goBack() {
              window.location.href = "/index.html#three";
            }
          </script>
          <div class="container">
            <header class="major">
              <h3>
                Using Transformer Intervention to Increase Compositionality in
                Small Language Models
              </h3>
              <p>Created for Stanford CS224N: Natural Language Processing</p>
            </header>
            Over the last several years, transformer language models have shown
            continual improvement, boasting massive training corpuses that are
            increasingly out of reach for individuals and smaller research
            teams. The BabyLM challenge proposes a task to train models using
            limited, child-directed text in a quantity similar to the amount of
            words encountered by a 13 year old. The goal of this challenge is to
            improve pretraining techniques and increase the accessibility of
            cutting-edge language modeling to a broader audience.
            <p>
              <a href="#" class="image fit"
                ><img src="/images/babylm.png" alt="Generic polymer image"
              /></a>
            </p>
            <h6>
              Alex Warstadt, Leshem Choshen, Aaron Mueller, Adina Williams,
              Ethan Wilcox, and Chengxu Zhuang. Call for papers - the babylm
              challenge: Sample-efficient pretraining on a developmen- tally
              plausible corpus, 2023.
            </h6>
            <br />
            To address the BabyLM challenge, this study investigates whether
            increasing the compositionality of transformer models improves the
            model's ability to understand linguistic structure. Specifically,
            this study addresses the following three questions:
            <br />
            <br />
            <ul>
              <li>
                How do transformer models behave when they are trained on small
                datasets, (as inspired by the limitations of human development)?
              </li>
              <li>
                How can we measure the compositional understanding of these
                transformer models?
              </li>
              <li>
                How can we encourage transformers to learn compositional
                representations of inputs to enhance their ability to
                generalize?
              </li>
            </ul>

            <h4>Background: Tree Based Composition</h4>
            <script
              type="text/javascript"
              async
              src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"
            ></script>
            Tree-based linguistic structures have been shown to be a common way
            for humans to understand language, and they have been correlated
            with increased compositional generalization in models (Murty, 2022).
            In a recent paper, Murty et al. introduced a novel metric, \(
            \mathbf{t_{\text{score}}} \), to measure how tree-structured a
            transformer model is. Using this metric, they demonstrated a
            correlation between a high \( \mathbf{t_{\text{score}}} \)
            (indicating tree-structuredness) and increased compositional
            generalization in transformer models.

            <span class="math display"
              >\[ t_{\text{score}} \triangleq \frac{1}{|\mathcal{D}|} \sum_{S
              \in \mathcal{D}} \left( \mathbb{E}_{T} \left[
              \operatorname{SCI}(S, T) \right] - \operatorname{SCI}\left( S,
              \widehat{T}_{\text{proj}}(S) \right) \right). \]</span
            >

            This metric searches for the optimal bracketing of an input sentence
            that maximizes the span contextual invariance (SCI) of each bracket.
            By using a similarity function to measure how similar the embedding
            of the bracketed sentence is to the original sentence embedding, we
            can determine a relative score for how effective the transformer is
            at learning heierarchical composition. A visual representation of a
            tranformer comparing a bracketed embedding to the original embedding
            can be seen in Figure 2 below.
            <br />
            <br />

            <p>
              <a href="#" class="image fit"
                ><img
                  src="/images/TreeChart.png"
                  alt="Illustration of how a sentence can be split up using a tree structured approach"
              /></a>
            </p>
            <h6>
              Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher D.
              Manning. Characterizing intrinsic compositionality in transformers
              with tree projections, 2022.
            </h6>
            <br />
            <h4>Approach</h4>
            In order to investigate the potential benefits of incorporating
            tree-structured computations into the transformer architectures, the
            study was split into a series of systematic steps. The detailed
            process for this study includes the following steps:
            <br />
            <br />
            <ul>
              <li>
                <strong><span>Step 1 | Baseline Language Model</span></strong
                >: Train GPT2-small architectures (6 and 12 encoder layers) on
                the small dataset (10M tokens).
              </li>

              <li>
                <strong><span>Step 2 | Model Evaluation</span></strong
                >: Evaluate perplexity and run syntactic evaluations on
                BLiMP<span style="font-size: 90%">[1]</span> to evaluate the
                models on common grammatical phenomena in English.
              </li>

              <li>
                <strong
                  ><span>Step 3 | Measure Tree-Structuredness</span></strong
                >: Incorporate the \( t_{\text{score}} \) metric into the GPT2
                transformer architecture to measure tree-structuredness during
                training.
              </li>

              <li>
                <strong
                  ><span
                    >Step 4 | Encouraging Tree-Structured Computations</span
                  ></strong
                >: We replaced the cross-entropy loss \(
                \mathcal{L}_{\text{entropy}} \) of GPT-2 by a regularized loss:
                \( \mathcal{L}_{\text{entropy}} + \lambda
                \mathcal{L}_{\text{tree}} \) to encourage the transformer to
                learn tree-structured representations of inputs.
              </li>
            </ul>
          </div>
        </section>
      </div>

      <!-- Footer -->
      <section id="footer">
        <div class="container">
          <ul class="copyright">
            <li><a href="#">Back to Top </a></li>
            <li>&copy; Untitled. All rights reserved.</li>
          </ul>
        </div>
      </section>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
