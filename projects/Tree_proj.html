<!DOCTYPE html>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html lang="en">
  <head>
    <title>Thomas Little</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="/assets/css/main.css" />
  </head>

  <body class="is-preload">
    <!-- Header -->
    <section id="header">
      <header>
        <span class="image avatar"
          ><img src="/images/avatar.jpg" alt=""
        /></span>
        <h1 id="logo"><a href="#">Thomas Little</a></h1>
        <p>
          MS Stanford University<br />
          A repository of my cool fun projects and some other information<br />
        </p>
      </header>
      <nav id="nav">
        <ul>
          <li><a href="/index.html#one">About</a></li>
          <li>
            <a href="/index.html#two" class="active">Projects</a>
          </li>
          <li><a href="/index.html#three">Certifications & Education</a></li>
        </ul>
        <!-- <ul>
          <li>
            <a href="GMM_project.html">Ground Motion Modeling using DNNs</a>
          </li>
          <li>
            <a href="GNN_Project.html"
              >Graph Neural Nets for Polymer Prediction</a
            >
          </li>
          <li>
            <a href="Tree_proj.html" class="active"
              >Increasing Compositionality in Small LMs</a
            >
          </li>
          <li>
            <a href="Nonlin_project.html">Nonlinear Structural Analysis</a>
          </li>
          <li><a href="/index.html">Home</a></li>
        </ul> -->
      </nav>
      <footer>
        <ul class="icons">
          <li>
            <a
              href="https://www.linkedin.com/in/thomas-little-0/"
              class="icon brands fa-linkedin"
              ><span class="label">LinkedIn</span></a
            >
          </li>
          <li>
            <a
              href="https://github.com/thomasjlittle"
              class="icon brands fa-github"
              ><span class="label">Github</span></a
            >
          </li>
          <li>
            <a
              href="mailto:thomasjlittle01@gmail.com"
              class="icon solid fa-envelope"
              ><span class="label">Email</span></a
            >
          </li>
        </ul>
      </footer>
    </section>

    <!-- Wrapper -->
    <div id="wrapper">
      <!-- Main -->
      <div id="main">
        <!-- One -->
        <section id="one">
          <button class="button" onclick="goBack()">Back</button>
          <script>
            function goBack() {
              window.location.href = "/index.html#two";
            }
          </script>
          <div class="container">
            <header class="major">
              <h3>
                Using Transformer Intervention to Increase Compositionality in
                Small Language Models
              </h3>
              <p>
                Adapted From a Project for Stanford CS224N: Natural Language
                Processing
              </p>
            </header>
            Over the last several years, transformer language models have shown
            continual improvement, boasting massive training corpuses that are
            increasingly out of reach for individuals and smaller research
            teams. The BabyLM challenge proposes a task to train models using
            limited, child-directed text in a quantity similar to the amount of
            words encountered by a 13 year old. The goal of this challenge is to
            improve pretraining techniques and increase the accessibility of
            cutting-edge language modeling to a broader audience.
            <p>
              <a class="image fit"
                ><img
                  src="/images/babylm.png"
                  alt="Generic polymer image"
                  style="margin-bottom: 0.5em"
              /></a>
            </p>
            <h6>
              Alex Warstadt, Leshem Choshen, Aaron Mueller, Adina Williams,
              Ethan Wilcox, and Chengxu Zhuang. Call for papers - the babylm
              challenge: Sample-efficient pretraining on a developmen- tally
              plausible corpus, 2023.
            </h6>
            <br />
            To address the BabyLM challenge, this study investigates whether
            increasing the compositionality of transformer models improves the
            model's ability to understand linguistic structure. Specifically,
            this study addresses the following three questions:
            <br />
            <br />
            <ul>
              <li>
                <strong><span>Question 1</span></strong
                >: How do transformer models behave when they are trained on
                small datasets, (as inspired by the limitations of human
                development)?
              </li>
              <br />
              <li>
                <strong><span>Question 2</span></strong
                >: How can we measure the compositional understanding of these
                transformer models?
              </li>
              <br />
              <li>
                <strong><span>Question 3</span></strong
                >: How can we encourage transformers to learn compositional
                representations of inputs to enhance their ability to
                generalize?
              </li>
            </ul>

            <h4>Background: Tree Based Composition</h4>
            <script
              type="text/javascript"
              async
              src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"
            ></script>
            Tree-based linguistic structures have been shown to be a common way
            for humans to understand language, and they have been correlated
            with increased compositional generalization in models (Murty et al.,
            2022). In a recent paper, Murty et al. introduced a novel metric, \(
            \mathbf{t_{\text{score}}} \), to measure how tree-structured a
            transformer model is. Using this metric, they demonstrated a
            correlation between a high \( \mathbf{t_{\text{score}}} \)
            (indicating tree-structuredness) and increased compositional
            generalization in transformer models.

            <span class="math display"
              >\[ t_{\text{score}} \triangleq \frac{1}{|\mathcal{D}|} \sum_{S
              \in \mathcal{D}} \left( \mathbb{E}_{T} \left[
              \operatorname{SCI}(S, T) \right] - \operatorname{SCI}\left( S,
              \widehat{T}_{\text{proj}}(S) \right) \right). \]</span
            >

            This metric searches for the optimal bracketing of an input sentence
            that maximizes the span contextual invariance (SCI) of each bracket.
            By using a similarity function to measure how similar the embedding
            of the bracketed sentence is to the original sentence embedding, we
            can determine a relative score for how effective the transformer is
            at learning heierarchical composition. A visual representation of a
            tranformer comparing a bracketed embedding to the original embedding
            can be seen in Figure 2 below.
            <br />
            <br />

            <figure>
              <img
                class="image fit"
                src="/images/TreeChart.png"
                alt="Illustration of how a sentence can be split up using a tree structured approach"
                style="margin-bottom: 0m"
              />
              <figcaption style="text-align: center">
                <h6>
                  Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and
                  Christopher D. Manning. Characterizing intrinsic
                  compositionality in transformers with tree projections, 2022.
                </h6>
              </figcaption>
            </figure>

            <br />
            <h4>Approach</h4>
            In order to investigate the potential benefits of incorporating
            tree-structured computations into the transformer architectures, the
            study was split into a series of systematic steps. The detailed
            process for this study includes the following steps:
            <br />
            <br />
            <ul>
              <li>
                <strong><span>Step 1 | Baseline Language Model</span></strong
                >: Train GPT2-small architectures (6 and 12 encoder layers) on
                the small dataset (10M tokens).
              </li>
              <br />
              <li>
                <strong><span>Step 2 | Model Evaluation</span></strong
                >: Evaluate perplexity and run syntactic evaluations on
                BLiMP<span style="font-size: 90%">[1]</span> to evaluate the
                models on common grammatical phenomena in English.
              </li>
              <br />
              <li>
                <strong
                  ><span>Step 3 | Measure Tree-Structuredness</span></strong
                >: Incorporate the \( t_{\text{score}} \) metric into the GPT2
                transformer architecture to measure tree-structuredness during
                training.
              </li>
              <br />
              <li>
                <strong
                  ><span
                    >Step 4 | Encouraging Tree-Structured Computations</span
                  ></strong
                >: We replaced the cross-entropy loss \(
                \mathcal{L}_{\text{entropy}} \) of GPT-2 by a regularized loss:
                \( \mathcal{L}_{\text{entropy}} + \lambda
                \mathcal{L}_{\text{tree}} \) to encourage the transformer to
                learn tree-structured representations of inputs.
              </li>
            </ul>
            To calculate the additional \( \mathcal{L}_{\text{tree}} \) loss
            term, we first determine a SCI score for all possible splits of the
            input sentence. We then recursively search each split to identify
            the optimal decomposition for the input sentence that maximizes the
            SCI score. Finally, we compare this optimal decomposition with the
            SCI score of a randomly selected split using the following formula:

            <span class="math display"
              >\[ \mathcal{L}_{\text{tree}} = max[(\text{SCI}_{random} + \beta -
              \text{SCI}_{best}), 0] \]
            </span>

            This aims to train the model to prefer the optimal splits over
            randomly selected splits (here \( \beta \) is a hyperparameter
            currently set to 0.1).
            <br />
            <br />

            <h4>Experimental Results</h4>
            To begin this study, the two baseline GPT2 architectures described
            above were evaluated. The difference in performance was found to be
            very minor, so moved forward using only the 6-layer encoder model
            for increased iteration speed. We then measured the model
            performance on perplexity, which can be seen in Figure 3 and found
            it to be decreasing over time. Next, we ran a ensemble of syntactic
            evaluations using BLiMP (the Benchmark of Linguistic Minimal Pairs)
            framework (Warstadt et al., 2020), the scores of which can be seen
            in Table 1 below.
            <br />
            <br />

            <div class="table-wrapper">
              <table>
                <thead>
                  <tr>
                    <th>Model</th>
                    <th>BLiMP Overall Score</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>GPT2-BabyLM-Baseline</td>
                    <td>0.69</td>
                  </tr>
                  <tr>
                    <td>GPT2-Pretrained</td>
                    <td>0.83</td>
                  </tr>
                  <tr>
                    <td>Human Performance</td>
                    <td>0.89</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <h6>Table 1. BLiMP Scores</h6>
            <br />
            BLiMP scores tests the model's ability to discern a grammatically
            incorrect sentence from a pair of sentences, making the random
            chance score equal to 50%. Lastly, the model \( t_{\text{score}} \)
            was calculated, and showed an increase in tree-structuredness as
            training progresed and model performance increased. This mirrors the
            correlation described by Murty et al. (2022), but still does not
            show causation.
            <br />
            <br />

            <p>
              <a class="image fit"
                ><img
                  src="/images/BaselineResults.png"
                  alt="Baseline results for GPT2 model"
              /></a>
            </p>
            <h6>Figure 3. Baseline Model Results</h6>
            <br />
            <h4>Discussion</h4>
            The updated GPT2 architecture with a tree-regularizing loss function
            was retrained on the same data as the baseline model. Its evaluation
            accuracy is plotted in orange below against the 6 layer encoder
            baseline model. Perplexity, BLiMP score, and \( t_{\text{score}} \)
            plots are omitted in this section as they show stochastic behavior
            as seen in Figure 4 below and do not provide further insight into
            the behavior of the model.
            <br />
            <br />
            <p>
              <a class="image fit"
                ><img
                  src="/images/evalAcc.png"
                  alt="Evaluation accuracy for baseline and tree-regularized GPT2 models"
              /></a>
            </p>
            <h6>Figure 4. Evaluation Accuracy</h6>
            <br />
            Astute observers may notice that the following figure does not
            describe a successful improvement in model performance. Even more
            astute observers may notice that the tree-regularized model was
            trained on far fewer epochs than the baseline model. The reason for
            both of these facts is simple: the tree-regularized model we trained
            was bad. It should be noted that the underlying principles
            propelling these tests are not inherently flawed, instead the issues
            likely arose from limiting factors imposed on our implementations.
            <br />
            <br />
            By checking the \( t_{\text{score}} \) of the model at various
            checkpoints, we can see that the model was learning to produce more
            tree-structured outputs. However, it is clear from the evaluation
            accuracies that this did not result in improved model performance.
            It is likely that the regularization factor \( \lambda \) was too
            large, leading the model to overfit on the tree-structuredness of
            the training data. Despite running the model with several different
            values of \( \lambda \), we were unable to find a value that
            resulted in improved model performance. The challenge with iterating
            on this variable stems form the algorithmic runtime of the recursive
            chart parsing used to calculate \( \mathcal{L}_{\text{tree}} \). The
            time complexity for this function is \(O(n^3) \), where \(n\) is
            related to the length of the input sequence as well as the number of
            input sentences analyzed per batch. Unfortunately, this means that
            the runtime of the model is incredibly slow. In attempts to reduce
            this runtime, we made additional simplifications that would speed up
            the calculations but would likely have a negative impact on model
            performance. The two main simplifications we made were truncating
            all input sentences to a maximum of 10 words and set our maximum
            sentence batch size to 50. A future goal of this study would be to
            quantify the impact of these assumptions by running the model with
            increasing words and batch sizes, as it is currently unknown how
            heavily this impacts the model performance.
            <br />
            <br />
            Both of these assumptions likely have very large impacts on model
            performance, as they increased the inherent stochasticity in the
            gradient updates, since each update is based on a very small
            percentage of the overall dataset. Accounting for the fact that our
            training dataset contained 650,000 input sentences, a batch size of
            50 is only 0.008 percent of the overall dataset. We attempted using
            1 percent of the overall dataset to calculate \(
            \mathcal{L}_{\text{tree}} \) at each step, however, training was
            estimated to take over 600 hours (again due to the \(O(n^3)\)
            runtime).
            <br />
            <br />
            <h4>Future Work</h4>
            Since it is clear from these tests that the model overfit to the
            tree-structuredness of the training data, it would be interesting to
            see how the model performs if we scaled up the regularization factor
            \( \lambda \) during training. If we started with a value of zero
            and slowly increased it over time, it may allow the model to learn a
            more comprehensive understanding of language before imposing the
            tree-structured constraint. This may result in a more robust model
            that is able to construct a more compositional understanding of
            language. Another drawback to the viability of this approach is its
            algorithmic runtime of the tree-regularizing loss function. As it is
            currently, using this method (even for small-scale datasets)
            presents challenges for individuals wishing to train models. It is
            possible to reduce the runtime to at least \(O(n^2)\), which would
            increase the feasibility of using this method, and would likely
            result in improvements over the performance seen in this study.
            Hopefully this avenue is explored in the future so that further
            research can be conducted on the impact of imposed tree-structured
            computations in transformer models and their abilities to gain
            increased compositional generalization.
            <br />
            <br />
            <h4>References</h4>
            [1] Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher
            D. Manning. Characterizing intrinsic compositionality in
            transformers with tree projections, 2022.
            <br />
            <br />
            [2] Alex Warstadt, Leshem Choshen, Aaron Mueller, Adina Williams,
            Ethan Wilcox, and Chengxu Zhuang. Call for papers - the babylm
            challenge: Sample-efficient pretraining on a developmentally
            plausible corpus, 2023.
            <br />
            <br />
            [3] Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei
            Peng, Sheng-Fu Wang, and Samuel R. Bowman. Blimp: The benchmark of
            linguistic minimal pairs for english. Transactions of the
            Association for Computational Linguistics, 8:377-392, 2020.
          </div>
        </section>
      </div>

      <!-- Footer -->
      <section id="footer">
        <div class="container">
          <ul class="copyright">
            <li><a href="#">Back to Top </a></li>
          </ul>
        </div>
      </section>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
